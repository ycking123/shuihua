# ============================================================================
# æ–‡ä»¶: url_crawler.py
# æ¨¡å—: backend
# èŒè´£: ä¼šè®® URL çˆ¬è™«ï¼Œè´Ÿè´£ä»è…¾è®¯ä¼šè®®ç­‰å¹³å°çˆ¬å–ä¼šè®®æ•°æ®
#
# ä¾èµ–å£°æ˜:
#   - å¤–éƒ¨: requests, logging, re, json, html, time, random, string, difflib, typing, urllib.parse
#   - æœ¬æ¨¡å—: backend.ai_handler (extract_todos_from_text)
#   - å¯é€‰: backend.crawl_with_browser (crawl_meeting_minutes) - PLAYWRIGHT_AVAILABLE æ ‡å¿—ä½
#
# ä¸»è¦æ¥å£:
#   - crawl_and_parse_meeting(url, cookies_str) -> Dict: ä¸»å…¥å£ï¼Œçˆ¬å–ä¼šè®®æ•°æ®
#   - crawl_meeting_api(share_url, user_cookie) -> Dict: API çˆ¬è™«æ–¹æ³•
#   - crawl_meeting_transcript_api(share_url, user_cookie) -> str: è·å–ä¼šè®®è½¬å†™
#   - crawl_meeting_summary_api(share_url, user_cookie) -> Dict: è·å–ä¼šè®®æ‘˜è¦
#
# ============================================================================

import requests
import logging
import re
import json
import html as html_lib
import time
import random
import string
from difflib import SequenceMatcher
from typing import Optional, Dict, Any, List, Tuple
from urllib.parse import urljoin, urlparse, parse_qs
from backend.ai_handler import extract_todos_from_text

# Playwright æµè§ˆå™¨çˆ¬è™«ä¸ºå¯é€‰ä¾èµ–ï¼ˆAPI çˆ¬è™«ä¸éœ€è¦ï¼‰
try:
    from backend.crawl_with_browser import crawl_meeting_minutes
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    crawl_meeting_minutes = None
    PLAYWRIGHT_AVAILABLE = False

logger = logging.getLogger("URLCrawler")

# ============================================================================
# æ–°çš„ API çˆ¬è™«æ–¹æ³• (åŸºäº tecent.py å’Œ summary.py)
# ============================================================================

def get_random_str(length=9):
    """ç”Ÿæˆéšæœºå­—ç¬¦ä¸²ç”¨äºæŒ‡çº¹æ ¡éªŒ"""
    return ''.join(random.choices(string.ascii_letters + string.digits, k=length))

def _maybe_decode_base64_text(value: str) -> str:
    if not value:
        return value
    try:
        import base64
        decoded = base64.b64decode(value).decode("utf-8")
        return decoded or value
    except Exception:
        return value

def get_all_recording_ids(share_url, user_cookie: Optional[str] = None):
    """
    è·å–ä¼šè®®æ‰€æœ‰å½•åˆ¶ç‰‡æ®µçš„ ID
    è¿”å›: (sharing_id, meeting_id, recording_list)
    """
    parsed_url = urlparse(share_url)
    query_params = parse_qs(parsed_url.query)
    sharing_id = query_params.get('id', [None])[0]
    
    api_url = "https://meeting.tencent.com/wemeet-tapi/v2/meetlog/public/detail/common-record-info"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Content-Type": "application/json",
        "Referer": share_url
    }
    if user_cookie:
        headers["Cookie"] = user_cookie
    payload = {"sharing_id": sharing_id, "is_single": False, "lang": "zh", "forward_cgi_path": "shares", "enter_from": "share"}

    try:
        response = requests.post(api_url, json=payload, headers=headers, timeout=10)
        res_data = response.json()
        data = res_data.get("data", {})
        
        recordings = data.get("recordings", [])
        recording_list = []
        for rec in recordings:
            recording_list.append({"id": rec.get("id"), "topic": rec.get("topic")})
        
        return sharing_id, data.get("meeting_info", {}).get("meeting_id"), recording_list
    except Exception as e:
        logger.error(f"è·å–å½•åˆ¶ ID å¤±è´¥: {e}")
        return None, None, []

def crawl_meeting_transcript_api(share_url, user_cookie: Optional[str] = None):
    """
    ä½¿ç”¨ API çˆ¬å–å®Œæ•´çš„ä¼šè®®è½¬å†™å†…å®¹ (åŸºäº tecent.py)
    è¿”å›: æŒ‰å‘è¨€äººåˆ†ç»„çš„è½¬å†™æ–‡æœ¬
    """
    sharing_id, meeting_id, recording_list = get_all_recording_ids(share_url, user_cookie)
    
    if not recording_list:
        logger.warning("æœªè·å–åˆ°å½•åˆ¶ç‰‡æ®µ")
        return ""

    detail_api_url = "https://meeting.tencent.com/wemeet-cloudrecording-webapi/v1/minutes/detail"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/145.0.0.0 Safari/537.36 Edg/145.0.0.0",
        "Referer": "https://meeting.tencent.com/"
    }
    if user_cookie:
        headers["Cookie"] = user_cookie

    transcript_lines = []
    last_speaker = None
    merged_content = ""

    for rec_info in recording_list:
        rec_id = rec_info['id']
        current_pid = "0"
        
        while True:
            params = {
                "id": sharing_id,
                "meeting_id": meeting_id,
                "recording_id": rec_id,
                "platform": "Web",
                "lang": "zh",
                "pid": current_pid,
                "minutes_version": 0,
                "limit": 50
            }
            
            try:
                response = requests.get(detail_api_url, headers=headers, params=params, timeout=10)
                res_json = response.json()
                
                data_root = res_json if "minutes" in res_json else res_json.get("data", {})
                minutes_obj = data_root.get("minutes", {})
                paragraphs = minutes_obj.get("paragraphs", [])
                
                if not paragraphs:
                    break

                for p in paragraphs:
                    speaker = p.get("speaker", {}).get("user_name", "æœªçŸ¥")
                    text = "".join(["".join([w.get("text", "") for w in s.get("words", [])]) for s in p.get("sentences", [])])

                    if speaker == last_speaker:
                        merged_content += text
                    else:
                        if last_speaker:
                            transcript_lines.append(f"ã€{last_speaker}ã€‘: {merged_content}")
                        last_speaker = speaker
                        merged_content = text

                server_has_more = data_root.get("has_more", minutes_obj.get("has_more", False))
                next_pid = data_root.get("next_pid", minutes_obj.get("next_pid", 0))

                if next_pid and str(next_pid) != "0" and str(next_pid) != str(current_pid):
                    current_pid = str(next_pid)
                elif paragraphs:
                    last_pid_in_page = str(paragraphs[-1].get("pid"))
                    if last_pid_in_page != current_pid:
                        current_pid = last_pid_in_page
                    else:
                        break
                else:
                    break
                    
            except Exception as e:
                logger.error(f"çˆ¬å–è½¬å†™ä¸­æ–­: {e}")
                break
    
    if last_speaker:
        transcript_lines.append(f"ã€{last_speaker}ã€‘: {merged_content}")
    
    return "\n\n".join(transcript_lines)

def get_meeting_params(share_url, user_cookie: Optional[str] = None):
    """é€šè¿‡åˆ†äº«é“¾æ¥è‡ªåŠ¨è·å– sharing_id, meeting_id å’Œ record_id"""
    parsed_url = urlparse(share_url)
    query_params = parse_qs(parsed_url.query)
    sharing_id = query_params.get('id', [None])[0]
    
    if not sharing_id:
        logger.error("æ— æ³•ä» URL ä¸­è§£æå‡º ID")
        return None, None, None

    api_url = "https://meeting.tencent.com/wemeet-tapi/v2/meetlog/public/detail/common-record-info"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Content-Type": "application/json",
        "Referer": share_url
    }
    if user_cookie:
        headers["Cookie"] = user_cookie
    payload = {
        "sharing_id": sharing_id, 
        "is_single": False, 
        "lang": "zh", 
        "forward_cgi_path": "shares", 
        "enter_from": "share"
    }

    try:
        response = requests.post(api_url, json=payload, headers=headers, timeout=10)
        res_data = response.json()
        data = res_data.get("data", {})
        meeting_id = data.get("meeting_info", {}).get("meeting_id")
        meeting_title = data.get("meeting_info", {}).get("subject", "ä¼šè®®çºªè¦")
        if meeting_title:
            meeting_title = _maybe_decode_base64_text(meeting_title)
        recordings = data.get("recordings", [])
        record_id = recordings[0].get("id") if recordings else None
        return sharing_id, meeting_id, record_id, meeting_title
    except Exception as e:
        logger.error(f"è·å–ä¼šè®®å‚æ•°å¤±è´¥: {e}")
        return None, None, None, None

def crawl_meeting_summary_api(share_url, user_cookie: Optional[str] = None):
    """
    ä½¿ç”¨ API çˆ¬å–ä¼šè®® AI æ‘˜è¦å’Œå¾…åŠ (åŸºäº summary.py)
    è¿”å›: { title, summary, transcript, todos, personal_todos }
    """
    result = get_meeting_params(share_url, user_cookie)
    if not result or not all(result[:3]):
        return None
    
    share_id, meeting_id, record_id, meeting_title = result

    nonce = get_random_str(9)
    timestamp = str(int(time.time() * 1000))
    trace_id = get_random_str(32).lower()

    api_url = (
        f"https://meeting.tencent.com/wemeet-tapi/v2/meetlog/public/record-detail/get-mul-summary-and-todo?"
        f"c_timestamp={timestamp}&c_nonce={nonce}&meeting_id={meeting_id}&trace-id={trace_id}&c_lang=zh-CN"
    )

    payload = {
        "record_id": record_id,
        "meeting_id": meeting_id,
        "lang": "zh",
        "share_id": share_id,
        "summary_type": 0
    }

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Content-Type": "application/json",
        "Referer": share_url
    }
    if user_cookie:
        headers["Cookie"] = user_cookie

    try:
        response = requests.post(api_url, headers=headers, json=payload, timeout=10)
        res_json = response.json()
        
        if res_json.get("code") != 0:
            logger.warning(f"API è¿”å›é”™è¯¯: {res_json.get('message')}")
            return None

        data = res_json.get("data", {})
        
        # è§£æç»“æœ
        summary_parts = []
        todos = []
        
        # 1. DeepSeek æ™ºèƒ½æ€»ç»“
        ds_summary = data.get("deepseek_summary", {}).get("topic_summary", {})
        if ds_summary:
            begin = ds_summary.get("begin_summary", "")
            if begin:
                summary_parts.append(f"ã€AI æ™ºèƒ½æ€»è§ˆã€‘\n{begin}")
            for point in ds_summary.get("sub_points", []):
                title = point.get('sub_point_title', '')
                summary_parts.append(f"\n# {title}")
                for item in point.get("sub_point_vec_items", []):
                    summary_parts.append(f" â€¢ {item.get('point', '')}")
        
        # 2. ç« èŠ‚æ¦‚è§ˆ
        chapters = data.get("chapter_summary", {}).get("summary_list", [])
        if chapters:
            chapter_text = "\nã€ç« èŠ‚å†…å®¹è¯¦æƒ…ã€‘"
            for idx, c in enumerate(chapters, 1):
                chapter_text += f"\n{idx}. {c.get('title', '')}\n   å†…å®¹: {c.get('summary', '')}"
            summary_parts.append(chapter_text)
        
        # 3. å‘è¨€äººæ€»ç»“
        speakers = data.get("speaker_summary", {}).get("speakers_opinions", [])
        if speakers:
            speaker_text = "\nã€å‘è¨€äººè§‚ç‚¹æ•´åˆã€‘"
            for s in speakers:
                speaker_id = s.get('speaker_id', 'æœªçŸ¥')
                speaker_text += f"\nå‘è¨€äºº: {speaker_id}"
                for sp in s.get("sub_points", []):
                    speaker_text += f"\n  [{sp.get('sub_point_title', '')}]"
                    for item in sp.get("sub_point_vec_items", []):
                        speaker_text += f"\n   - {item.get('point', '')}"
            summary_parts.append(speaker_text)
        
        # 4. å¾…åŠäº‹é¡¹
        todo_list = data.get("todo", {}).get("todo_list", [])
        for t in todo_list:
            todos.append({
                "title": t.get("todo_name", "å¾…åŠäº‹é¡¹"),
                "description": t.get("todo_name", ""),
                "assignee": "å¾…å®š",
                "priority": "normal",
                "due_date": ""
            })
        
        summary_text = "\n".join(summary_parts)
        
        logger.info(f"âœ… API çˆ¬å–æˆåŠŸ: è·å–æ‘˜è¦ {len(summary_text)} å­—ç¬¦, å¾…åŠ {len(todos)} æ¡")
        
        return {
            "title": meeting_title or "ä¼šè®®çºªè¦",
            "summary": summary_text,
            "transcript": "",  # æ‘˜è¦ API ä¸è¿”å›å®Œæ•´è½¬å†™
            "todos": todos,
            "personal_todos": todos  # å…¼å®¹æ—§ç»“æ„
        }
        
    except Exception as e:
        logger.error(f"çˆ¬å–æ‘˜è¦å¼‚å¸¸: {e}")
        return None

def crawl_meeting_api(share_url, user_cookie: Optional[str] = None):
    """
    æ–°çš„ API çˆ¬è™«ä¸»å…¥å£ - åŒæ—¶è·å–æ‘˜è¦å’Œè½¬å†™
    """
    logger.info(f"ğŸš€ ä½¿ç”¨ API çˆ¬è™«æ¨¡å¼çˆ¬å–: {share_url}")
    
    # 1. å…ˆè·å–æ‘˜è¦å’Œå¾…åŠ
    summary_result = crawl_meeting_summary_api(share_url, user_cookie)
    
    # 2. å†è·å–å®Œæ•´è½¬å†™
    transcript = crawl_meeting_transcript_api(share_url, user_cookie)
    
    # 3. åˆå¹¶ç»“æœ
    if summary_result:
        summary_result["transcript"] = transcript
        # å¦‚æœ API æ²¡æœ‰å¾…åŠï¼Œä»è½¬å†™ä¸­æå–
        if not summary_result.get("todos") and transcript:
            ai_result = extract_todos_from_text(transcript)
            if ai_result:
                summary_result["summary"] = summary_result.get("summary", "") or ai_result.get("summary", "")
                summary_result["todos"] = ai_result.get("task_list", [])
        # ç”Ÿæˆ personal_todos
        if transcript:
            summary_result["personal_todos"] = build_personal_todos(transcript, summary_result.get("todos", []))
        return summary_result
    elif transcript:
        # åªæœ‰è½¬å†™ï¼Œæ²¡æœ‰æ‘˜è¦
        ai_result = extract_todos_from_text(transcript)
        return {
            "title": "ä¼šè®®çºªè¦",
            "summary": ai_result.get("summary", "") if ai_result else "",
            "transcript": transcript,
            "todos": ai_result.get("task_list", []) if ai_result else [],
            "personal_todos": build_personal_todos(transcript, ai_result.get("task_list", []) if ai_result else [])
        }
    
    return None

def extract_meeting_url(text: str) -> Optional[str]:
    """
    ä»æ–‡æœ¬ä¸­æå–è…¾è®¯ä¼šè®®/ä¼ä¸šå¾®ä¿¡æ–‡æ¡£çš„ URL
    """
    # åŒ¹é…å¸¸è§çš„ä¼šè®®é“¾æ¥æ ¼å¼
    # https://meeting.tencent.com/p/xxx
    # https://meeting.tencent.com/wework/cloud-record/share?id=xxx
    url_pattern = r'(https?://(?:meeting\.tencent\.com|docs\.qq\.com|doc\.weixin\.qq\.com)/[^\s]+)'
    match = re.search(url_pattern, text)
    if match:
        return match.group(1)
    return None

def extract_json_object(text: str, start_index: int) -> Optional[str]:
    """Find the matching closing brace for a JSON object starting at start_index"""
    brace_count = 0
    in_string = False
    escape = False
    
    for i in range(start_index, len(text)):
        char = text[i]
        if in_string:
            if escape:
                escape = False
            elif char == '\\':
                escape = True
            elif char == '"':
                in_string = False
        else:
            if char == '"':
                in_string = True
            elif char == '{':
                brace_count += 1
            elif char == '}':
                brace_count -= 1
                if brace_count == 0:
                    return text[start_index:i+1]
    return None

def parse_meeting_html(html: str) -> Dict[str, Any]:
    """
    è§£æè…¾è®¯ä¼šè®® HTMLï¼Œæå– serverData ä¸­çš„å…ƒæ•°æ®
    """
    result = {
        "title": "æœªçŸ¥ä¼šè®®",
        "meeting_id": "",
        "duration": 0,
        "minutes_text": "",
        "recordings": []
    }
    
    try:
        # æŸ¥æ‰¾ Next.js çš„ hydration æ•°æ®: self.__next_f.push([1,"..."])
        pushes = re.finditer(r'self\.__next_f\.push\(\[(.*?)\]\)', html)
        
        for match in pushes:
            inner = match.group(1)
            parts = inner.split(',', 1)
            if len(parts) < 2: continue
            
            json_str_raw = parts[1].strip()
            
            # å°è¯•è§£æ JSON å­—ç¬¦ä¸²
            if json_str_raw.startswith('"') and json_str_raw.endswith('"'):
                try:
                    content = json.loads(json_str_raw)
                    if "serverData" in content:
                        sd_match = re.search(r'"serverData":(\{.*?\})', content)
                        if sd_match:
                            start = content.find('"serverData":') + len('"serverData":')
                            obj_str = extract_json_object(content, start)
                            if obj_str:
                                sd = json.loads(obj_str)
                                
                                # æå–å…³é”®ä¿¡æ¯
                                if "meeting_info" in sd:
                                    subject = sd["meeting_info"].get("subject", "")
                                    # å°è¯• Base64 è§£ç æ ‡é¢˜ (è…¾è®¯ä¼šè®®æ ‡é¢˜å¸¸ä¸º Base64)
                                    try:
                                        import base64
                                        decoded_subject = base64.b64decode(subject).decode('utf-8')
                                        result["title"] = decoded_subject
                                    except:
                                        result["title"] = subject
                                    
                                    result["meeting_id"] = sd["meeting_info"].get("meeting_id", "")
                                
                                result["duration"] = sd.get("total_recording_duration", 0)
                                result["recordings"] = sd.get("recordings", [])
                                
                                # æ£€æŸ¥æ˜¯å¦æœ‰çºªè¦æ–‡æœ¬ (ç›®å‰é€šå¸¸ä¸ºç©ºï¼Œéœ€è¦ API)
                                if "smart_minutes" in sd:
                                    result["minutes_text"] = str(sd["smart_minutes"])
                                
                                logger.info(f"âœ… æˆåŠŸæå–ä¼šè®®å…ƒæ•°æ®: {result['title']}")
                                return result
                except Exception as e:
                    continue
                    
    except Exception as e:
        logger.error(f"âŒ è§£æ HTML å¤±è´¥: {e}")
        
    return result

def extract_json_block(text: str, start_index: int) -> Optional[str]:
    stack = []
    in_string = False
    escape = False
    for i in range(start_index, len(text)):
        char = text[i]
        if in_string:
            if escape:
                escape = False
            elif char == '\\':
                escape = True
            elif char == '"':
                in_string = False
        else:
            if char == '"':
                in_string = True
            elif char in "{[":
                stack.append(char)
            elif char in "}]":
                if not stack:
                    return None
                last = stack.pop()
                if not stack:
                    return text[start_index:i + 1]
    return None

def extract_next_payloads(html: str) -> List[str]:
    payloads = []
    pushes = re.finditer(r'self\.__next_f\.push\(\[(.*?)\]\)', html)
    for match in pushes:
        inner = match.group(1)
        parts = inner.split(',', 1)
        if len(parts) < 2:
            continue
        json_str_raw = parts[1].strip()
        if json_str_raw.startswith('"') and json_str_raw.endswith('"'):
            try:
                payloads.append(json.loads(json_str_raw))
            except Exception:
                continue
    return payloads

def extract_next_data_json(html: str) -> Optional[str]:
    match = re.search(r'__NEXT_DATA__[^>]*>(.*?)</script>', html, re.DOTALL)
    if not match:
        return None
    raw = match.group(1)
    try:
        return json.dumps(json.loads(raw), ensure_ascii=False)
    except Exception:
        return None

def extract_server_data_objects(texts: List[str]) -> List[Dict[str, Any]]:
    server_data_list = []
    for text in texts:
        if not text or "serverData" not in text:
            continue
        idx = text.find('"serverData":')
        while idx != -1:
            start = idx + len('"serverData":')
            obj_str = extract_json_object(text, start)
            if obj_str:
                try:
                    server_data_list.append(json.loads(obj_str))
                except Exception:
                    pass
            idx = text.find('"serverData":', idx + len('"serverData":'))
    return server_data_list

def find_text_by_keys(obj: Any, key_keywords: List[str], min_length: int = 60) -> List[str]:
    results = []
    if isinstance(obj, dict):
        for k, v in obj.items():
            if isinstance(v, str):
                if any(kw in k.lower() for kw in key_keywords) and len(v) >= min_length:
                    results.append(v)
            else:
                results.extend(find_text_by_keys(v, key_keywords, min_length))
    elif isinstance(obj, list):
        for item in obj:
            results.extend(find_text_by_keys(item, key_keywords, min_length))
    return results

def pick_best_text(texts: List[str]) -> str:
    if not texts:
        return ""
    def score(s: str) -> int:
        zh_count = len(re.findall(r'[\u4e00-\u9fa5]', s))
        return len(s) + zh_count * 2
    return max(texts, key=score)

def strip_html_tags(html: str) -> str:
    if "<" not in html:
        return html
    cleaned = re.sub(r"<(script|style)[^>]*>.*?</\1>", " ", html, flags=re.DOTALL | re.IGNORECASE)
    cleaned = re.sub(r"<[^>]+>", " ", cleaned)
    cleaned = html_lib.unescape(cleaned)
    cleaned = re.sub(r"[ \t\r\f\v]+", " ", cleaned)
    return cleaned.strip()

def extract_title_from_html(html: str) -> str:
    match = re.search(r"<title[^>]*>(.*?)</title>", html, re.DOTALL | re.IGNORECASE)
    if match:
        return html_lib.unescape(match.group(1)).strip()
    return ""

def extract_json_values(text: str, key: str) -> List[str]:
    values = []
    pattern = rf'"{re.escape(key)}"\s*:\s*'
    for match in re.finditer(pattern, text):
        idx = match.end()
        while idx < len(text) and text[idx].isspace():
            idx += 1
        if idx >= len(text):
            continue
        if text[idx] in "{[":
            block = extract_json_block(text, idx)
            if block:
                values.append(block)
        elif text[idx] == '"':
            i = idx + 1
            escape = False
            while i < len(text):
                char = text[i]
                if escape:
                    escape = False
                elif char == '\\':
                    escape = True
                elif char == '"':
                    values.append(text[idx + 1:i])
                    break
                i += 1
    return values

def normalize_transcript(text: str) -> str:
    if not text:
        return ""
    
    # 1. åŸºç¡€æ¸…æ´—
    text = text.replace("\u3000", " ").replace("\r", " ").replace("\t", " ")
    
    # 2. ç§»é™¤å¸¸è§çš„ UI å¯¼èˆªè¯ (å°¤å…¶æ˜¯å‡ºç°åœ¨å¼€å¤´æˆ–å•ç‹¬ä¸€è¡Œçš„)
    ui_words = ["è¿”å›", "æ›´å¤š", "åˆ†äº«", "æ”¶è—", "æœç´¢", "å…¨éƒ¨", "åªçœ‹", "å¯¼å‡º", "ç¿»è¯‘", "å€é€Ÿ"]
    lines = text.split('\n')
    cleaned_lines = []
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        # è·³è¿‡æçŸ­çš„ UI è¯
        if len(line) <= 4 and line in ui_words:
            continue
        # è·³è¿‡çº¯æ—¶é—´æˆ³ (å¦‚æœä¸æ˜¯è·Ÿåœ¨åå­—åé¢çš„è¯ï¼Œä¸è¿‡è¿™é‡Œç®€å•å¤„ç†)
        # if re.match(r'^\d{1,2}:\d{2}$', line):
        #    continue
            
        cleaned_lines.append(line)
    
    text = " ".join(cleaned_lines)
    
    # 3. æ ‡ç‚¹ç¬¦å·è§„èŒƒåŒ–ä¸åˆ†è¡Œ
    text = re.sub(r"\s{2,}", " ", text)
    text = text.replace("ã€‚", "ã€‚\n").replace("ï¼", "ï¼\n").replace("ï¼Ÿ", "ï¼Ÿ\n")
    text = re.sub(r"\n{2,}", "\n", text)
    
    return text.strip()

def split_speaker_segments(text: str) -> List[Tuple[str, str]]:
    segments = []
    pattern = re.compile(r'([\u4e00-\u9fa5A-Za-z]{1,10})\s*(\d{1,2}:\d{2})')
    matches = list(pattern.finditer(text))
    if not matches:
        return [("", text)]
    for i, match in enumerate(matches):
        speaker = match.group(1)
        start = match.end()
        end = matches[i + 1].start() if i + 1 < len(matches) else len(text)
        content = text[start:end].strip()
        segments.append((speaker, content))
    return segments

def normalize_name(name: str) -> str:
    if "Sender" in name or "å‘é€è€…" in name:
        return "Senderï¼ˆå‘é€è€…ï¼‰"
    name = re.sub(r"[^\u4e00-\u9fa5A-Za-z0-9]", "", name)
    name = re.sub(r"(å…ˆç”Ÿ|å¥³å£«|æ€»|ç»ç†|è€å¸ˆ|åŒå­¦|ä¸»ä»»|è€æ¿)$", "", name)
    return name.strip()

def cluster_names(names: List[str]) -> Dict[str, str]:
    normalized = []
    mapping = {}
    for name in names:
        norm = normalize_name(name)
        if not norm:
            continue
        matched = None
        for base in normalized:
            ratio = SequenceMatcher(None, norm, base).ratio()
            if ratio >= 0.85 or norm in base or base in norm:
                matched = base
                break
        if not matched:
            normalized.append(norm)
            matched = norm
        mapping[name] = matched
    return mapping

def extract_task_sentences(text: str) -> List[str]:
    keywords = [
        "è´Ÿè´£", "è·Ÿè¿›", "å®Œæˆ", "æäº¤", "æ•´ç†", "æ±‡æ€»", "å‡†å¤‡", "æ¨è¿›",
        "å¯¹æ¥", "å®‰æ’", "è½å®", "è¾“å‡º", "å¤ç›˜", "ç¡®è®¤", "æµ‹è¯•", "ä¸Šçº¿",
        "ä¼˜åŒ–", "éœ€æ±‚", "ä¿®å¤", "äº¤ä»˜", "é…åˆ", "æ”¯æŒ", "å›ä¼ "
    ]
    sentences = re.split(r"[ã€‚ï¼ï¼Ÿ\n]", text)
    tasks = []
    for s in sentences:
        s = s.strip()
        if len(s) < 6:
            continue
        if any(k in s for k in keywords):
            tasks.append(s)
    return tasks

def build_personal_todos(transcript: str, meeting_todos: List[dict]) -> List[dict]:
    personal = []
    segments = split_speaker_segments(transcript)
    speaker_names = [s for s, _ in segments if s]
    name_map = cluster_names(speaker_names)

    for t in meeting_todos or []:
        if isinstance(t, str):
            personal.append({
                "title": t,
                "description": t,
                "assignee": "Senderï¼ˆå‘é€è€…ï¼‰",
                "priority": "normal",
                "due_date": ""
            })
            continue
        assignee = t.get("assignee") or "Senderï¼ˆå‘é€è€…ï¼‰"
        assignee = name_map.get(assignee, normalize_name(assignee)) or "Senderï¼ˆå‘é€è€…ï¼‰"
        personal.append({
            "title": t.get("title") or "ä¼šè®®å¾…åŠ",
            "description": t.get("description") or t.get("title") or "",
            "assignee": assignee,
            "priority": t.get("priority") or "normal",
            "due_date": t.get("due_date") or ""
        })

    if personal:
        return personal

    for speaker, content in segments:
        assignee = name_map.get(speaker, normalize_name(speaker)) or "Senderï¼ˆå‘é€è€…ï¼‰"
        for s in extract_task_sentences(content):
            title = s[:32]
            personal.append({
                "title": title,
                "description": s,
                "assignee": assignee,
                "priority": "normal",
                "due_date": ""
            })
    return personal

def parse_meeting_page(html: str) -> Dict[str, Any]:
    candidate_texts = [html]
    candidate_texts.extend([p for p in extract_next_payloads(html) if p])
    next_data = extract_next_data_json(html)
    if next_data:
        candidate_texts.append(next_data)
    server_data_list = extract_server_data_objects(candidate_texts)

    title = ""
    for text in candidate_texts:
        if not title:
            title = extract_title_from_html(text)
        for sd in extract_json_values(text, "serverData"):
            try:
                sd_json = json.loads(sd)
                meeting_info = sd_json.get("meeting_info") or {}
                subject = meeting_info.get("subject") or ""
                if subject:
                    try:
                        import base64
                        title = base64.b64decode(subject).decode("utf-8")
                    except Exception:
                        title = subject
            except Exception:
                continue

    transcript_candidates = []
    for text in candidate_texts:
        for key in ["transcript", "transcription", "asr", "minutes_text", "text"]:
            transcript_candidates.extend(extract_json_values(text, key))
    transcript = ""
    for t in transcript_candidates:
        if isinstance(t, str) and len(t) > len(transcript):
            transcript = t
    if not transcript and server_data_list:
        key_keywords = ["transcript", "transcription", "asr", "subtitle", "minutes", "summary", "text", "content", "speech"]
        text_hits = []
        for sd in server_data_list:
            text_hits.extend(find_text_by_keys(sd, key_keywords))
        transcript = pick_best_text(text_hits)
    if not transcript:
        transcript = strip_html_tags(html)

    transcript = normalize_transcript(transcript)
    ai_result = extract_todos_from_text(transcript) if transcript else None
    summary = ""
    todos = []
    if ai_result:
        summary = ai_result.get("summary", "")
        todos = ai_result.get("task_list", []) or []

    personal_todos = build_personal_todos(transcript, todos)

    return {
        "title": title or "ä¼šè®®çºªè¦",
        "summary": summary or "",
        "transcript": transcript or "",
        "todos": todos or [],
        "personal_todos": personal_todos or []
    }

def crawl_and_parse_meeting_browser(url: str, cookies_str: Optional[str] = None) -> Optional[Dict[str, Any]]:
    """
    [å¤‡ç”¨æ–¹æ³•] ä½¿ç”¨ Playwright æµè§ˆå™¨çˆ¬è™«è·å–ä¼šè®®å†…å®¹
    """
    if not PLAYWRIGHT_AVAILABLE:
        logger.error("âŒ Playwright æœªå®‰è£…ï¼Œæµè§ˆå™¨çˆ¬è™«ä¸å¯ç”¨ã€‚è¯·è¿è¡Œ: pip install playwright && playwright install chromium")
        return None
    
    try:
        logger.info(f"ğŸ”„ åˆ‡æ¢åˆ°æµè§ˆå™¨çˆ¬è™«æ¨¡å¼ (Playwright) çˆ¬å–: {url}")
        # è°ƒç”¨åŸºäº pc.py é€»è¾‘çš„æµè§ˆå™¨çˆ¬è™«
        result = crawl_meeting_minutes(url, cookies_str)
        
        if not result:
            logger.warning("âš ï¸ æµè§ˆå™¨çˆ¬è™«æœªè¿”å›ç»“æœ")
            return None
            
        # è¡¥å…… personal_todos ç”Ÿæˆé€»è¾‘ (å¤ç”¨æœ¬æ–‡ä»¶ä¸­çš„å‡½æ•°)
        # å¦‚æœçˆ¬è™«è¿”å›ç»“æœä¸­æ²¡æœ‰ personal_todosï¼Œä½†æœ‰è½¬å†™æ–‡æœ¬å’Œå¾…åŠï¼Œåˆ™è‡ªåŠ¨ç”Ÿæˆ
        if not result.get("personal_todos") and result.get("transcript"):
            logger.info("ğŸ”¨ æ­£åœ¨åŸºäºè½¬å†™å†…å®¹ç”Ÿæˆä¸ªäººå¾…åŠ...")
            result["personal_todos"] = build_personal_todos(result["transcript"], result.get("todos", []))
            
        return result
        
    except Exception as e:
        logger.error(f"âŒ æµè§ˆå™¨çˆ¬è™«å¤±è´¥: {e}")
        return None

def crawl_and_parse_meeting(url: str, cookies_str: Optional[str] = None) -> Optional[Dict[str, Any]]:
    """
    ä¸»çˆ¬è™«å…¥å£ - ä¼˜å…ˆä½¿ç”¨ API çˆ¬è™«ï¼Œå¤±è´¥æ—¶å›é€€åˆ°æµè§ˆå™¨çˆ¬è™«
    
    çˆ¬å–ç­–ç•¥ï¼š
    1. é¦–å…ˆå°è¯• API çˆ¬è™« (crawl_meeting_api) - ç›´æ¥è°ƒç”¨è…¾è®¯ä¼šè®® APIï¼Œé€Ÿåº¦å¿«
    2. å¦‚æœ API çˆ¬è™«å¤±è´¥ï¼Œå›é€€åˆ°æµè§ˆå™¨çˆ¬è™« (crawl_and_parse_meeting_browser) - ä½¿ç”¨ Playwright æ¨¡æ‹Ÿæµè§ˆå™¨
    """
    # 1. ä¼˜å…ˆå°è¯• API çˆ¬è™«
    # ç­–ç•¥ A: å¦‚æœæœ‰ cookieï¼Œå…ˆè¯•å¸¦ cookie
    if cookies_str:
        try:
            logger.info("å°è¯•å¸¦ Cookie è¿›è¡Œ API çˆ¬å–...")
            result = crawl_meeting_api(url, cookies_str)
            if result:
                logger.info("âœ… å¸¦ Cookie API çˆ¬è™«æˆåŠŸ")
                return result
        except Exception as e:
            logger.warning(f"âš ï¸ å¸¦ Cookie API çˆ¬è™«å¤±è´¥: {e}")

    # ç­–ç•¥ B: å°è¯•æ—  Cookie API çˆ¬å– (å…¬å¼€é“¾æ¥)
    try:
        logger.info("å°è¯•æ—  Cookie API çˆ¬å–...")
        # ä¼ å…¥ None ä½œä¸º cookieï¼Œrequests ä¼šå¿½ç•¥ None çš„ header
        result = crawl_meeting_api(url, None)
        if result:
            logger.info("âœ… æ—  Cookie API çˆ¬è™«æˆåŠŸ")
            return result
    except Exception as e:
        logger.warning(f"âš ï¸ æ—  Cookie API çˆ¬è™«å¤±è´¥: {e}")
    
    # 2. å›é€€åˆ°æµè§ˆå™¨çˆ¬è™«
    logger.info("ğŸ”„ å›é€€åˆ°æµè§ˆå™¨çˆ¬è™«æ¨¡å¼...")
    return crawl_and_parse_meeting_browser(url, cookies_str)

# åŸæœ‰çš„é™æ€çˆ¬å–é€»è¾‘ä¿ç•™ä½†ä¸ä½¿ç”¨
def crawl_and_parse_meeting_legacy(url: str, cookies_str: Optional[str] = None) -> Optional[Dict[str, Any]]:
    html = fetch_content_with_cookies(url, cookies_str)

def fetch_content_with_cookies(url: str, cookies_str: Optional[str]) -> Optional[str]:
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }
    if cookies_str:
        headers["Cookie"] = cookies_str

    try:
        logger.info(f"ğŸ•·ï¸ æ­£åœ¨å°è¯•çˆ¬å– URL: {url}")
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        content = response.text
        
        # 1. æ£€æµ‹ JavaScript é‡å®šå‘
        # window.location.replace("...") æˆ– window.location.href = "..."
        redirect_pattern = r'window\.location\.(?:replace|href)\s*\(?\s*["\']([^"\']+)["\']'
        redirect_match = re.search(redirect_pattern, content)
        if redirect_match:
            new_url = redirect_match.group(1)
            logger.info(f"ğŸ”„ æ£€æµ‹åˆ° JS é‡å®šå‘ï¼Œæ­£åœ¨è·³è½¬è‡³: {new_url}")
            # å¤„ç†ç›¸å¯¹è·¯å¾„
            if new_url.startswith('/'):
                 # ç®€å•æå–åŸŸå
                 from urllib.parse import urljoin
                 new_url = urljoin(url, new_url)
            
            # é€’å½’è°ƒç”¨ (é˜²æ­¢æ­»å¾ªç¯å¯ä»¥åŠ ä¸ªè®¡æ•°å™¨ï¼Œè¿™é‡Œç®€å•å¤„ç†)
            return fetch_content_with_cookies(new_url, cookies_str)

        logger.info(f"âœ… çˆ¬å–æˆåŠŸï¼Œè·å–åˆ° {len(content)} å­—ç¬¦")
        return content
    except Exception as e:
        logger.error(f"âŒ çˆ¬å–å¤±è´¥: {e}")
        return None
